{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.26.4 protobuf==4.25.3 mediapipe==0.10.20 opencv-contrib-python==4.8.1.78 pandas==2.1.4 joblib==1.4.2 tqdm==4.66.4\n",
        "!pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 pytorch-lightning==2.3.3\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IDAlY3_5uVsb",
        "outputId": "80185411-150c-4729-a8bf-7f2d8a50def9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf==4.25.3\n",
            "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting mediapipe==0.10.20\n",
            "  Downloading mediapipe-0.10.20-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Collecting opencv-contrib-python==4.8.1.78\n",
            "  Downloading opencv_contrib_python-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting pandas==2.1.4\n",
            "  Downloading pandas-2.1.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting joblib==1.4.2\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting tqdm==4.66.4\n",
            "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.20) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.20) (25.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.20) (25.2.10)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.20) (0.5.3)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.20) (0.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.20) (3.10.0)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe==0.10.20)\n",
            "  Downloading sounddevice-0.5.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.20) (0.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas==2.1.4) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas==2.1.4) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.12/dist-packages (from pandas==2.1.4) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas==2.1.4) (1.17.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->mediapipe==0.10.20) (1.17.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe==0.10.20) (0.5.3)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe==0.10.20) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe==0.10.20) (1.16.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.20) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.20) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.20) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.20) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.20) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.20) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.20) (3.2.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe==0.10.20) (2.22)\n",
            "Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mediapipe-0.10.20-cp312-cp312-manylinux_2_28_x86_64.whl (35.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_contrib_python-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.1.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.2-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: tqdm, protobuf, numpy, joblib, sounddevice, pandas, opencv-contrib-python, mediapipe\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.5.2\n",
            "    Uninstalling joblib-1.5.2:\n",
            "      Successfully uninstalled joblib-1.5.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: opencv-contrib-python\n",
            "    Found existing installation: opencv-contrib-python 4.12.0.88\n",
            "    Uninstalling opencv-contrib-python-4.12.0.88:\n",
            "      Successfully uninstalled opencv-contrib-python-4.12.0.88\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.1.4 which is incompatible.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 2.1.4 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "dataproc-spark-connect 0.8.3 requires tqdm>=4.67, but you have tqdm 4.66.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.3 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 2.1.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "xarray 2025.8.0 requires pandas>=2.2, but you have pandas 2.1.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed joblib-1.4.2 mediapipe-0.10.20 numpy-1.26.4 opencv-contrib-python-4.8.1.78 pandas-2.1.4 protobuf-4.25.3 sounddevice-0.5.2 tqdm-4.66.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "numpy"
                ]
              },
              "id": "ab81bfe60977412db5736a291f9e477d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.3.1\n",
            "  Downloading torch-2.3.1-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchvision==0.18.1\n",
            "  Downloading torchvision-0.18.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting torchaudio==2.3.1\n",
            "  Downloading torchaudio-2.3.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting pytorch-lightning==2.3.3\n",
            "  Downloading pytorch_lightning-2.3.3-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.1)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.1)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.1)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.1)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.1)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.1)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.1)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.1)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.1)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.1)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.18.1) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.18.1) (11.3.0)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning==2.3.3) (4.66.4)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning==2.3.3) (6.0.2)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning==2.3.3)\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning==2.3.3) (25.0)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning==2.3.3)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1) (12.6.85)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning==2.3.3) (3.12.15)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning==2.3.3) (75.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.3.1) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.3.1) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning==2.3.3) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning==2.3.3) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning==2.3.3) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning==2.3.3) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning==2.3.3) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning==2.3.3) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning==2.3.3) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning==2.3.3) (3.10)\n",
            "Downloading torch-2.3.1-cp312-cp312-manylinux1_x86_64.whl (779.1 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m519.7/779.1 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:20\u001b[0m^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pAr50Cfs9RJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import mediapipe as mp\n",
        "import cv2\n",
        "from collections import defaultdict\n",
        "from joblib import Parallel, delayed\n",
        "from tqdm import tqdm\n",
        "import ast\n",
        "import os\n",
        "import csv\n",
        "import re\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBhGUkMVs9RK"
      },
      "source": [
        "Load the videos to videos_list.csv (columns: file (path), label, gloss, video name, actor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nS6HPC-ns9RL"
      },
      "outputs": [],
      "source": [
        "folder_path = r'10 class 28 actor (different size)'\n",
        "csv_file_path = 'videos_list.csv'\n",
        "labels_file_path = '1_1000_label.csv'\n",
        "final_file_path = 'temp_videos_list.csv'\n",
        "\n",
        "label_to_gloss = {}\n",
        "with open(labels_file_path, mode='r', encoding='utf-8') as labels_file:\n",
        "    csv_reader = csv.DictReader(labels_file)\n",
        "    for row in csv_reader:\n",
        "        label = int(row['id_label_in_documents'])\n",
        "        gloss = row['name']\n",
        "        label_to_gloss[label] = gloss\n",
        "\n",
        "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "    csv_writer.writerow(['file', 'label', 'gloss', 'video_name', 'actor'])\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.lower().endswith(('.mp4', '.mkv', '.avi', '.mov', '.flv', '.wmv')):\n",
        "            actor = filename.split('_')[0]\n",
        "\n",
        "            match = re.search(r'_(\\d+)\\.', filename)\n",
        "            if match:\n",
        "                label = int(match.group(1))\n",
        "                gloss = label_to_gloss.get(label, 'Unknown')\n",
        "            else:\n",
        "                label = 'N/A'\n",
        "                gloss = 'Unknown'\n",
        "\n",
        "            if label != 200:\n",
        "                full_filename = os.path.join(folder_path, filename)\n",
        "                csv_writer.writerow([full_filename, label, gloss, filename, actor])\n",
        "\n",
        "print(f'Video names have been written to {csv_file_path}')\n",
        "\n",
        "# Find min label\n",
        "with open(csv_file_path, mode='r', newline='', encoding='utf-8') as csv_file:\n",
        "    csv_reader = csv.DictReader(csv_file)\n",
        "    labels = [int(row[\"label\"]) for row in csv_reader if row[\"label\"].isdigit()]\n",
        "    min_label = min(labels) if labels else None\n",
        "\n",
        "print(\"Minimum label:\", min_label)\n",
        "\n",
        "# Normalize labels\n",
        "with open(csv_file_path, mode='r', newline='', encoding='utf-8') as csv_file, \\\n",
        "     open(final_file_path, mode='w', newline='', encoding='utf-8') as final_file:\n",
        "\n",
        "    csv_reader = csv.DictReader(csv_file)\n",
        "    fieldnames = csv_reader.fieldnames\n",
        "\n",
        "    csv_writer = csv.DictWriter(final_file, fieldnames=fieldnames)\n",
        "    csv_writer.writeheader()\n",
        "\n",
        "    for row in csv_reader:\n",
        "        if row['label'].isdigit():  # Check if label is a digit before converting\n",
        "            row['label'] = str(int(row['label']) - min_label)\n",
        "        csv_writer.writerow(row)\n",
        "\n",
        "# Replace the original file with the updated file\n",
        "os.replace(final_file_path, csv_file_path)\n",
        "\n",
        "print(\"Labels have been updated and saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OZzIS_4s9RN"
      },
      "source": [
        "Number of labels in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoQLcNvks9RO"
      },
      "outputs": [],
      "source": [
        "num_labels = len(set(labels))\n",
        "print(num_labels)\n",
        "print(set(labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Balance the number of videos"
      ],
      "metadata": {
        "id": "DXyLaOfqqfOe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGdYRgFWs9RQ"
      },
      "outputs": [],
      "source": [
        "import csv, os, re, random, collections, pathlib, copy\n",
        "\n",
        "INPUT_CSV  = \"videos_list.csv\"\n",
        "OUTPUT_CSV = \"videos_list_balanced.csv\"\n",
        "SEED       = 42\n",
        "random.seed(SEED)\n",
        "\n",
        "with open(INPUT_CSV, newline='', encoding='utf-8') as f:\n",
        "    rows          = list(csv.DictReader(f))\n",
        "    fieldnames    = rows[0].keys()\n",
        "\n",
        "actors           = sorted({r['actor'] for r in rows})\n",
        "total_actors     = len(actors)\n",
        "print(f\"Detected {total_actors} actors: {actors}\")\n",
        "\n",
        "by_label = collections.defaultdict(list)\n",
        "for r in rows:\n",
        "    by_label[r['label']].append(r)\n",
        "\n",
        "def patch_row(base_row, missing_actor):\n",
        "    new_row          = copy.deepcopy(base_row)\n",
        "    new_actor_code   = f\"{int(missing_actor):02d}\"\n",
        "    new_row['actor'] = new_actor_code\n",
        "\n",
        "    def swap_code(s):\n",
        "        return re.sub(r'^[0-9]{2}(?=_)', new_actor_code, s)\n",
        "\n",
        "    new_row['video_name'] = swap_code(new_row['video_name'])\n",
        "    new_row['file']       = swap_code(new_row['file'])\n",
        "    return new_row\n",
        "\n",
        "for label, label_rows in list(by_label.items()):\n",
        "    present_actors = {r['actor'] for r in label_rows}\n",
        "    missing        = [a for a in actors if a not in present_actors]\n",
        "    for ma in missing:\n",
        "        donor = random.choice(label_rows)\n",
        "        synthetic = patch_row(donor, ma)\n",
        "        label_rows.append(synthetic)\n",
        "    by_label[label] = label_rows\n",
        "\n",
        "final_rows = []\n",
        "for label, label_rows in by_label.items():\n",
        "    seen = {}\n",
        "    uniques, extras = [], []\n",
        "    for r in label_rows:\n",
        "        actor = r['actor']\n",
        "        if actor not in seen:\n",
        "            seen[actor] = True\n",
        "            uniques.append(r)\n",
        "        else:\n",
        "            extras.append(r)\n",
        "    while len(uniques) < total_actors and extras:\n",
        "        uniques.append(extras.pop())\n",
        "    final_rows.extend(uniques[:total_actors])\n",
        "\n",
        "pathlib.Path(OUTPUT_CSV).write_text('', encoding='utf-8')\n",
        "with open(OUTPUT_CSV, 'w', newline='', encoding='utf-8') as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    writer.writerows(final_rows)\n",
        "\n",
        "print(f\"\\nBalanced file written to: {OUTPUT_CSV}\")\n",
        "\n",
        "counter = collections.Counter(r['label'] for r in final_rows)\n",
        "print(\"\\nCount per label (ascending):\")\n",
        "for lab, cnt in sorted(counter.items(), key=lambda x: x[1]):\n",
        "    print(f\"{lab:>4} : {cnt}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2pyp8QNs9RS"
      },
      "source": [
        "Extract keypoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wla2Gous9RS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import mediapipe as mp\n",
        "import cv2\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from joblib import Parallel, delayed\n",
        "from tqdm import tqdm\n",
        "\n",
        "mp_holistic = mp.solutions.holistic\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "hand_landmarks = ['INDEX_FINGER_DIP', 'INDEX_FINGER_MCP', 'INDEX_FINGER_PIP', 'INDEX_FINGER_TIP',\n",
        "                  'MIDDLE_FINGER_DIP', 'MIDDLE_FINGER_MCP', 'MIDDLE_FINGER_PIP', 'MIDDLE_FINGER_TIP',\n",
        "                  'PINKY_DIP', 'PINKY_MCP', 'PINKY_PIP', 'PINKY_TIP', 'RING_FINGER_DIP', 'RING_FINGER_MCP',\n",
        "                  'RING_FINGER_PIP', 'RING_FINGER_TIP', 'THUMB_CMC', 'THUMB_IP', 'THUMB_MCP', 'THUMB_TIP', 'WRIST']\n",
        "pose_landmarks = ['LEFT_ANKLE', 'LEFT_EAR', 'LEFT_ELBOW', 'LEFT_EYE', 'LEFT_EYE_INNER', 'LEFT_EYE_OUTER',\n",
        "                  'LEFT_FOOT_INDEX', 'LEFT_HEEL', 'LEFT_HIP', 'LEFT_INDEX', 'LEFT_KNEE', 'LEFT_PINKY',\n",
        "                  'LEFT_SHOULDER', 'LEFT_THUMB', 'LEFT_WRIST', 'MOUTH_LEFT', 'MOUTH_RIGHT', 'NOSE',\n",
        "                  'RIGHT_ANKLE', 'RIGHT_EAR', 'RIGHT_ELBOW', 'RIGHT_EYE', 'RIGHT_EYE_INNER', 'RIGHT_EYE_OUTER',\n",
        "                  'RIGHT_FOOT_INDEX', 'RIGHT_HEEL', 'RIGHT_HIP', 'RIGHT_INDEX', 'RIGHT_KNEE', 'RIGHT_PINKY',\n",
        "                  'RIGHT_SHOULDER', 'RIGHT_THUMB', 'RIGHT_WRIST']\n",
        "\n",
        "def extract_keypoint(video_path, label, actor):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    keypoint_dict = defaultdict(list)\n",
        "    count = 0\n",
        "\n",
        "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            count += 1\n",
        "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            results = holistic.process(image)\n",
        "\n",
        "            if results.right_hand_landmarks:\n",
        "                for idx, landmark in enumerate(results.right_hand_landmarks.landmark):\n",
        "                    keypoint_dict[f\"{hand_landmarks[idx]}_right_x\"].append(landmark.x)\n",
        "                    keypoint_dict[f\"{hand_landmarks[idx]}_right_y\"].append(landmark.y)\n",
        "                    keypoint_dict[f\"{hand_landmarks[idx]}_right_z\"].append(landmark.z)\n",
        "            else:\n",
        "                for idx in range(len(hand_landmarks)):\n",
        "                    keypoint_dict[f\"{hand_landmarks[idx]}_right_x\"].append(0)\n",
        "                    keypoint_dict[f\"{hand_landmarks[idx]}_right_y\"].append(0)\n",
        "                    keypoint_dict[f\"{hand_landmarks[idx]}_right_z\"].append(0)\n",
        "\n",
        "            if results.left_hand_landmarks:\n",
        "                for idx, landmark in enumerate(results.left_hand_landmarks.landmark):\n",
        "                    keypoint_dict[f\"{hand_landmarks[idx]}_left_x\"].append(landmark.x)\n",
        "                    keypoint_dict[f\"{hand_landmarks[idx]}_left_y\"].append(landmark.y)\n",
        "                    keypoint_dict[f\"{hand_landmarks[idx]}_left_z\"].append(landmark.z)\n",
        "            else:\n",
        "                for idx in range(len(hand_landmarks)):\n",
        "                    keypoint_dict[f\"{hand_landmarks[idx]}_left_x\"].append(0)\n",
        "                    keypoint_dict[f\"{hand_landmarks[idx]}_left_y\"].append(0)\n",
        "                    keypoint_dict[f\"{hand_landmarks[idx]}_left_z\"].append(0)\n",
        "\n",
        "            if results.pose_landmarks:\n",
        "                for idx, landmark in enumerate(results.pose_landmarks.landmark):\n",
        "                    keypoint_dict[f\"{pose_landmarks[idx]}_x\"].append(landmark.x)\n",
        "                    keypoint_dict[f\"{pose_landmarks[idx]}_y\"].append(landmark.y)\n",
        "                    keypoint_dict[f\"{pose_landmarks[idx]}_z\"].append(landmark.z)\n",
        "            else:\n",
        "                for idx in range(len(pose_landmarks)):\n",
        "                    keypoint_dict[f\"{pose_landmarks[idx]}_x\"].append(0)\n",
        "                    keypoint_dict[f\"{pose_landmarks[idx]}_y\"].append(0)\n",
        "                    keypoint_dict[f\"{pose_landmarks[idx]}_z\"].append(0)\n",
        "\n",
        "        keypoint_dict[\"frame\"] = count\n",
        "        keypoint_dict[\"video_path\"] = video_path\n",
        "        keypoint_dict[\"label\"] = label\n",
        "        keypoint_dict[\"actor\"] = actor\n",
        "\n",
        "        return keypoint_dict\n",
        "\n",
        "def process_videos():\n",
        "    csv_file = f\"videos_list_balanced.csv\"\n",
        "    data = pd.read_csv(csv_file)\n",
        "\n",
        "    keypoints_list = Parallel(n_jobs=-1)(\n",
        "        delayed(extract_keypoint)(row['file'], row['label'], row['actor']) for index, row in tqdm(data.iterrows(), total=len(data), desc=\"Processing videos\", leave=False)\n",
        "    )\n",
        "\n",
        "    keypoints_df = pd.DataFrame(keypoints_list)\n",
        "    keypoints_df.to_csv(f\"vsl{num_labels}_keypoints.csv\", index=False)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    process_videos()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLxQM1WRs9RU"
      },
      "source": [
        "Interpolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jwH6ZCbs9RU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "from tqdm import tqdm\n",
        "\n",
        "def find_index(array):\n",
        "    for i, num in enumerate(array):\n",
        "        if num != 0:\n",
        "            return i\n",
        "\n",
        "def curl_skeleton(array):\n",
        "    if sum(array) == 0:\n",
        "        return array\n",
        "    for i, location in enumerate(array):\n",
        "        if location != 0:\n",
        "            continue\n",
        "        else:\n",
        "            if i == 0 or i == len(array) - 1:\n",
        "                continue\n",
        "            else:\n",
        "                if array[i + 1] != 0:\n",
        "                    array[i] = float((array[i - 1] + array[i + 1]) / 2)\n",
        "                else:\n",
        "                    if sum(array[i:]) == 0:\n",
        "                        continue\n",
        "                    else:\n",
        "                        j = find_index(array[i + 1:])\n",
        "                        array[i] = float(((1 + j) * array[i - 1] + 1 * array[i + 1 + j]) / (2 + j))\n",
        "    return array\n",
        "\n",
        "def interpolate_keypoints(input_file, output_file, body_identifiers):\n",
        "    train_data = pd.read_csv(input_file)\n",
        "    output_df = train_data.copy()\n",
        "\n",
        "    for index, video in tqdm(train_data.iterrows(), total=train_data.shape[0]):\n",
        "        for identifier in body_identifiers:\n",
        "            # Interpolate the x and y keypoints\n",
        "            x_values = curl_skeleton(ast.literal_eval(video[identifier + \"_x\"]))\n",
        "            y_values = curl_skeleton(ast.literal_eval(video[identifier + \"_y\"]))\n",
        "\n",
        "            output_df.at[index, identifier + \"_x\"] = str(x_values)\n",
        "            output_df.at[index, identifier + \"_y\"] = str(y_values)\n",
        "\n",
        "    output_df.to_csv(output_file, index=False)\n",
        "    print(f\"Interpolated keypoints saved to {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_file_path = f\"vsl{num_labels}_keypoints.csv\"\n",
        "    output_file_path = f\"vsl{num_labels}_interpolated_keypoints.csv\"\n",
        "\n",
        "    hand_landmarks = [\n",
        "        'INDEX_FINGER_DIP', 'INDEX_FINGER_MCP', 'INDEX_FINGER_PIP', 'INDEX_FINGER_TIP',\n",
        "        'MIDDLE_FINGER_DIP', 'MIDDLE_FINGER_MCP', 'MIDDLE_FINGER_PIP', 'MIDDLE_FINGER_TIP',\n",
        "        'PINKY_DIP', 'PINKY_MCP', 'PINKY_PIP', 'PINKY_TIP',\n",
        "        'RING_FINGER_DIP', 'RING_FINGER_MCP', 'RING_FINGER_PIP', 'RING_FINGER_TIP',\n",
        "        'THUMB_CMC', 'THUMB_IP', 'THUMB_MCP', 'THUMB_TIP', 'WRIST'\n",
        "    ]\n",
        "    HAND_IDENTIFIERS = [id + \"_right\" for id in hand_landmarks] + [id + \"_left\" for id in hand_landmarks]\n",
        "    POSE_IDENTIFIERS = [\"RIGHT_SHOULDER\", \"LEFT_SHOULDER\", \"LEFT_ELBOW\", \"RIGHT_ELBOW\"]\n",
        "    body_identifiers = HAND_IDENTIFIERS + POSE_IDENTIFIERS\n",
        "\n",
        "    interpolate_keypoints(input_file_path, output_file_path, body_identifiers)\n",
        "\n",
        "    # Load interpolated data and store them in numpy files\n",
        "    train_data = pd.read_csv(output_file_path)\n",
        "    frames = 80\n",
        "\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    for video_index, video in tqdm(train_data.iterrows(), total=train_data.shape[0]):\n",
        "        T = len(ast.literal_eval(video[\"INDEX_FINGER_DIP_right_x\"]))\n",
        "        current_row = np.empty(shape=(2, T, len(body_identifiers), 1))\n",
        "\n",
        "        for index, identifier in enumerate(body_identifiers):\n",
        "            data_keypoint_preprocess_x = ast.literal_eval(video[identifier + \"_x\"])\n",
        "            current_row[0, :, index, :] = np.asarray(data_keypoint_preprocess_x).reshape(T, 1)\n",
        "\n",
        "            data_keypoint_preprocess_y = ast.literal_eval(video[identifier + \"_y\"])\n",
        "            current_row[1, :, index, :] = np.asarray(data_keypoint_preprocess_y).reshape(T, 1)\n",
        "\n",
        "        if T < frames:\n",
        "            target = np.zeros(shape=(2, frames, len(body_identifiers), 1))\n",
        "            target[:, :T, :, :] = current_row\n",
        "        else:\n",
        "            target = current_row[:, :frames, :, :]\n",
        "\n",
        "        data.append(target)\n",
        "        labels.append(int(video[\"label\"]))\n",
        "\n",
        "    keypoint_data = np.stack(data, axis=0)\n",
        "    label_data = np.stack(labels, axis=0)\n",
        "    np.save(f'vsl{num_labels}_data_preprocess.npy', keypoint_data)\n",
        "    np.save(f'vsl{num_labels}_label_preprocess.npy', label_data)\n",
        "    print(\"Data processing and saving completed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTwRQPsws9RU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "a = np.load(f'vsl{num_labels}_data_preprocess.npy')\n",
        "b = np.load(f'vsl{num_labels}_label_preprocess.npy')\n",
        "\n",
        "print(a.shape)\n",
        "print(b.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDDf65Ghs9RU"
      },
      "source": [
        "Do K-Folds and store the keypoints in numpy files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q74FTAdls9RW"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "def k_fold_cross_validation(train_data, keypoint_data, label_data, num_labels, k_folds, destination_folder=\"numpy_files\"):\n",
        "    os.makedirs(destination_folder, exist_ok=True)\n",
        "\n",
        "    actors = train_data['actor'].unique()\n",
        "    print(f\"Number of actors: {len(actors)}\")\n",
        "    print('-----------------------------------------------------')\n",
        "\n",
        "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    actor_to_indices = {actor: train_data.index[train_data['actor'] == actor].tolist() for actor in actors}\n",
        "    folds = [[] for _ in range(k_folds)]\n",
        "\n",
        "    for fold, (train_actors, test_actors) in enumerate(kf.split(actors)):\n",
        "        train_actors = actors[train_actors]\n",
        "        test_actors = actors[test_actors]\n",
        "\n",
        "        for actor in test_actors:\n",
        "            folds[fold].extend(actor_to_indices[actor])\n",
        "\n",
        "        tqdm.write(f\"Fold {fold+1}: {len(folds[fold])} test samples\")\n",
        "\n",
        "    # Iterate over each fold to create train-test splits\n",
        "    for fold in range(k_folds):\n",
        "        test_indices = folds[fold]\n",
        "        train_indices = [idx for f in range(k_folds) if f != fold for idx in folds[f]]\n",
        "\n",
        "        X_train, X_test = keypoint_data[train_indices], keypoint_data[test_indices]\n",
        "        y_train = np.array(label_data[train_indices], dtype=np.int64)\n",
        "        y_test = np.array(label_data[test_indices], dtype=np.int64)\n",
        "\n",
        "        np.save(os.path.join(destination_folder, f'vsl{num_labels}_data_fold{fold+1}_train.npy'), X_train)\n",
        "        np.save(os.path.join(destination_folder, f'vsl{num_labels}_label_fold{fold+1}_train.npy'), y_train)\n",
        "        np.save(os.path.join(destination_folder, f'vsl{num_labels}_data_fold{fold+1}_test.npy'), X_test)\n",
        "        np.save(os.path.join(destination_folder, f'vsl{num_labels}_label_fold{fold+1}_test.npy'), y_test)\n",
        "\n",
        "        tqdm.write(f\"Processed and saved vsl{num_labels} fold {fold+1} successfully.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_file_path = f\"vsl{num_labels}_interpolated_keypoints.csv\"\n",
        "    train_data = pd.read_csv(input_file_path)\n",
        "\n",
        "    keypoint_data = np.load(f'vsl{num_labels}_data_preprocess.npy')\n",
        "    label_data = np.load(f'vsl{num_labels}_label_preprocess.npy')\n",
        "\n",
        "    num_labels = len(np.unique(label_data))\n",
        "\n",
        "    k_folds = 20\n",
        "    k_fold_cross_validation(train_data, keypoint_data, label_data, num_labels, k_folds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eh3EEjIs9RW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "a = np.load(f'numpy_files/vsl{num_labels}_data_fold2_test.npy')\n",
        "b = np.load(f'numpy_files/vsl{num_labels}_data_fold2_train.npy')\n",
        "\n",
        "print(a.shape)\n",
        "print(b.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8WAPtB4s9RX"
      },
      "source": [
        "train based on AUTSL with different folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XG5r8GBYs9RX"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from feeder import FeederINCLUDE\n",
        "from aagcn import Model\n",
        "from augumentation import Rotate, Compose\n",
        "from pytorch_lightning.utilities.migration import pl_legacy_patch\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    k_folds = 20\n",
        "    config = {'batch_size': 64, 'learning_rate': 0.0137296, 'weight_decay': 0.000150403}\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    best_accuracy = 0.0\n",
        "    best_fold = -1\n",
        "\n",
        "    for fold in range(k_folds):\n",
        "        print(f\"Starting fold {fold + 1}/{k_folds}\")\n",
        "        train_data_path = os.path.join(\"numpy_files\", f'vsl{num_labels}_data_fold{fold+1}_train.npy')\n",
        "        train_label_path = os.path.join(\"numpy_files\", f'vsl{num_labels}_label_fold{fold+1}_train.npy')\n",
        "        val_data_path = os.path.join(\"numpy_files\", f'vsl{num_labels}_data_fold{fold+1}_test.npy')\n",
        "        val_label_path = os.path.join(\"numpy_files\", f'vsl{num_labels}_label_fold{fold+1}_test.npy')\n",
        "\n",
        "        transforms = Compose([\n",
        "            Rotate(15, 80, 25, (0.5, 0.5))\n",
        "        ])\n",
        "\n",
        "        train_dataset = FeederINCLUDE(\n",
        "            data_path=train_data_path,\n",
        "            label_path=train_label_path,\n",
        "            transform=transforms\n",
        "        )\n",
        "        val_dataset = FeederINCLUDE(\n",
        "            data_path=val_data_path,\n",
        "            label_path=val_label_path\n",
        "        )\n",
        "\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
        "        val_dataloader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "\n",
        "        model = Model(num_class=num_labels, num_point=46, num_person=1, in_channels=2,\n",
        "                      graph_args={\"layout\": \"mediapipe_two_hand\", \"strategy\": \"spatial\"},\n",
        "                      learning_rate=config['learning_rate'], weight_decay=config['weight_decay'])\n",
        "\n",
        "        # Path pre-trained checkpoint file on AUTSL\n",
        "        checkpoint_path = \"autsl_vsl199-aagcn-fold=7-v1.ckpt\"\n",
        "\n",
        "        with pl_legacy_patch():\n",
        "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "        state_dict = checkpoint['state_dict']\n",
        "        filtered_state_dict = {k: v for k, v in state_dict.items() if not k.startswith('fc.')}\n",
        "        model.load_state_dict(filtered_state_dict, strict=False)\n",
        "\n",
        "        callbacks = [\n",
        "            ModelCheckpoint(\n",
        "                dirpath=\"checkpoints\",\n",
        "                monitor=\"valid_accuracy\",\n",
        "                mode=\"max\",\n",
        "                every_n_epochs=1,\n",
        "                filename=f'autsl_vsl{num_labels}-aagcn-fold={fold+1}'\n",
        "            ),\n",
        "        ]\n",
        "\n",
        "        trainer = pl.Trainer(max_epochs=100, accelerator=\"auto\", check_val_every_n_epoch=1,\n",
        "                             devices=1, callbacks=callbacks)\n",
        "\n",
        "        trainer.fit(model, train_dataloader, val_dataloader)\n",
        "        val_accuracy = trainer.callback_metrics['valid_accuracy'].item()\n",
        "        print(f\"Fold {fold + 1} finished with validation accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "        if val_accuracy > best_accuracy:\n",
        "            best_accuracy = val_accuracy\n",
        "            best_fold = fold + 1\n",
        "\n",
        "    print(f\"The highest validation accuracy achieved is {best_accuracy:.4f} from fold {best_fold}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdOkbOLxs9RX"
      },
      "outputs": [],
      "source": [
        "print(f\"The highest validation accuracy achieved of autsl vsl{num_labels} is {best_accuracy:.4f} from fold {best_fold}.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
